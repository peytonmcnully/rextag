# Schema Discovery & External Table Pipeline Design

## Problem

We need to pre-screen zipped geodatabases in GCS to discover their datasets, layers, and schemas before running the ELT pipeline. The discovered schemas should become dbt source definitions and staging models that create BigQuery external tables reading directly from hive-partitioned GCS paths.

## Solution

Add a `rextag scan` command that inspects geodatabase zips and generates dbt files. Modify `rextag extract` to write output to hive-partitioned GCS paths. External tables in BigQuery read the data directly — no BQ load jobs needed.

## Architecture

```
gs://siteselect-dbt/rextagsource/data_drop=2026-01/*.zip
    |
    v
[rextag scan]
    |-- Download each zip to temp
    |-- Fiona: list layers, read schemas
    |-- Detect geometry type per layer
    |-- Generate dbt _sources.yml + stg_*.sql
    v
dbt_project/models/staging/{dataset}/
    |-- _sources.yml (column definitions + meta.rename)
    |-- stg_{dataset}_{layer}.sql (external table DDL)

[user reviews/edits generated dbt files]

[rextag extract --config config.yml]
    |-- Download zips, convert to geojsonl/jsonl
    |-- Upload to hive-partitioned GCS:
    v
gs://siteselect-dbt/staged/{dataset}/{layer}/data_drop=2026-01/data.{geojsonl|jsonl}

[dbt run]
    |-- External tables read from GCS via hive wildcard
    |-- data_drop partition column auto-exposed
    v
BigQuery external tables (with GEOGRAPHY from json_extension=geojson)
```

## Scan Command

```
rextag scan --prefix gs://siteselect-dbt/rextagsource/data_drop=2026-01/ \
            --output-dir dbt_project/models/staging/
```

### Behavior

1. List all `*.zip` blobs under the given prefix
2. For each zip:
   - Download to temp dir
   - Unzip — expect `{filename}.zip` contains `{filename}.gdb`
   - Dataset name = `{filename}` (lowercased, sanitized)
   - Open each layer with Fiona to get: layer name, geometry type, field names + types
3. Generate dbt files per dataset in `{output_dir}/{dataset}/`

### Output Structure

```
dbt_project/models/staging/
  county_parcels/
    _sources.yml
    stg_county_parcels_parcels.sql
    stg_county_parcels_owners.sql
  zoning_data/
    _sources.yml
    stg_zoning_data_zones.sql
```

## Generated dbt Source YAML

```yaml
version: 2

sources:
  - name: county_parcels
    description: "Raw geodata from county_parcels.gdb"
    tables:
      - name: parcels
        description: "Layer: parcels (Polygon)"
        external:
          location: "gs://siteselect-dbt/staged/county_parcels/parcels/data_drop=*/"
          options:
            format: JSON
            json_extension: GEOJSON
            hive_partition_uri_prefix: "gs://siteselect-dbt/staged/county_parcels/parcels/"
        columns:
          - name: geometry
            description: "GeoJSON geometry (Polygon)"
            data_type: GEOGRAPHY
          - name: OBJECTID
            description: "Source: OBJECTID (int)"
            data_type: INT64
            meta:
              rename: null  # Set to target name to rename, e.g. "object_id"
          - name: PARCEL_NUM
            data_type: STRING
          - name: _loaded_at
            data_type: TIMESTAMP
          - name: _source_file
            data_type: STRING
          - name: _layer_name
            data_type: STRING
```

## Generated Staging SQL

### Layer with geometry (uses geojsonl + json_extension=geojson)

```sql
-- stg_county_parcels_parcels.sql
-- Auto-generated by rextag scan. Edit column renames in _sources.yml meta.

{{
    config(
        materialized='external_table',
    )
}}

select * from {{ source('county_parcels', 'parcels') }}
```

### Layer without geometry (uses jsonl)

```sql
-- stg_county_parcels_owners.sql

{{
    config(
        materialized='external_table',
    )
}}

select * from {{ source('county_parcels', 'owners') }}
```

Note: External table configuration (URIs, format, hive partitioning) is defined in the `_sources.yml` `external` block, which is how dbt-external-tables works.

## Modified Extract Command

```
rextag extract --config config.yml
```

### Changes from current behavior

1. **No BigQuery load** — removed. External tables handle reads.
2. **Hive-partitioned output paths**: `gs://bucket/staged/{dataset}/{layer}/data_drop={value}/data.{ext}`
3. **Auto-detect data_drop** from source URI prefix (parses `data_drop=2026-01` from path)
4. **File extension** based on geometry: `.geojsonl` if layer has geometry, `.jsonl` otherwise
5. **Output filename** is always `data.geojsonl` or `data.jsonl` (not layer name)

### GCS output layout

```
gs://siteselect-dbt/staged/
  county_parcels/
    parcels/
      data_drop=2026-01/
        data.geojsonl
    owners/
      data_drop=2026-01/
        data.jsonl
  zoning_data/
    zones/
      data_drop=2026-01/
        data.geojsonl
```

## Updated Config

```yaml
gcs:
  staging_bucket: "siteselect-dbt"
  staging_prefix: "staged/"

scan:
  source_prefix: "gs://siteselect-dbt/rextagsource/data_drop=2026-01/"
  dbt_output_dir: "dbt_project/models/staging/"

sources:
  - name: "county_parcels"
    uri: "gs://siteselect-dbt/rextagsource/data_drop=2026-01/county_parcels.zip"
  - name: "zoning_data"
    uri: "gs://siteselect-dbt/rextagsource/data_drop=2026-01/zoning_data.zip"
```

## data_drop Auto-Detection

Parse `data_drop=YYYY-MM` from the source URI:

```python
# gs://siteselect-dbt/rextagsource/data_drop=2026-01/county_parcels.zip
# -> data_drop = "2026-01"

import re
match = re.search(r'data_drop=([^/]+)', uri)
data_drop = match.group(1) if match else None
```

## File Extension Logic

```python
def get_file_extension(fiona_schema: dict) -> str:
    """Determine output file extension based on geometry type."""
    geom_type = fiona_schema.get("geometry")
    if geom_type and geom_type != "None":
        return "geojsonl"
    return "jsonl"
```

## Modules to Create/Modify

| Module | Action | Description |
|--------|--------|-------------|
| `rextag/scan.py` | CREATE | Schema discovery, dbt file generation |
| `rextag/cli.py` | MODIFY | Add `scan` command, modify `extract` for hive paths |
| `rextag/config.py` | MODIFY | Add scan config, data_drop parsing |
| `rextag/extract.py` | MODIFY | Add geometry detection, hive path output |
| `rextag/load.py` | MODIFY | Remove BQ load, keep GCS upload only |
| `dbt_project/packages.yml` | MODIFY | Add dbt-external-tables dependency |
| `tests/test_scan.py` | CREATE | Tests for scan module |

## Key Design Decisions

1. **External tables over native BQ tables** — Avoids data duplication, hive partitioning gives automatic data_drop column, new drops auto-appear.
2. **json_extension=geojson for geometry layers** — BigQuery natively parses GeoJSON into GEOGRAPHY at read time. No need for ST_GEOGFROMGEOJSON().
3. **Column renames in dbt YAML meta** — Single source of truth, co-located with column definitions.
4. **data_drop auto-detected from source path** — No manual flag needed, derived from the hive partition in the GCS prefix.
5. **Scan is separate from extract** — Allows user to review and edit generated dbt files before data lands.
