"""Schema discovery â€” inspect geodatabases and build a catalog of datasets/layers/schemas."""

from dataclasses import dataclass, field
from pathlib import Path

import fiona
import yaml

from rextag.extract import has_geometry
from rextag.schema import fiona_type_to_bq


@dataclass
class LayerInfo:
    """Discovered metadata for a single geodatabase layer."""

    name: str
    geometry_type: str | None
    fiona_schema: dict

    @property
    def file_extension(self) -> str:
        """File extension based on geometry presence: geojsonl or jsonl."""
        return "geojsonl" if has_geometry(self.fiona_schema) else "jsonl"

    @property
    def bq_columns(self) -> list[dict]:
        """Column definitions mapped to BigQuery types."""
        cols = []
        if has_geometry(self.fiona_schema):
            cols.append({
                "name": "geometry",
                "data_type": "GEOGRAPHY",
                "source_type": self.geometry_type,
            })
        for col_name, fiona_type in self.fiona_schema["properties"].items():
            cols.append({
                "name": col_name,
                "data_type": fiona_type_to_bq(fiona_type),
                "source_type": fiona_type,
            })
        cols.append({"name": "_loaded_at", "data_type": "TIMESTAMP", "source_type": None})
        cols.append({"name": "_source_file", "data_type": "STRING", "source_type": None})
        cols.append({"name": "_layer_name", "data_type": "STRING", "source_type": None})
        return cols


@dataclass
class DatasetInfo:
    """Discovered metadata for a geodatabase (one zip file)."""

    name: str
    layers: list[LayerInfo] = field(default_factory=list)


def inspect_geodatabase(gdb_path: Path, dataset_name: str) -> DatasetInfo:
    """Inspect a geodatabase and return metadata about all its layers."""
    layer_names = fiona.listlayers(gdb_path)
    layers = []

    for layer_name in layer_names:
        with fiona.open(gdb_path, layer=layer_name) as collection:
            schema = collection.schema
            geom_type = schema.get("geometry")
            if geom_type is not None and str(geom_type) == "None":
                geom_type = None

            layers.append(LayerInfo(
                name=layer_name,
                geometry_type=str(geom_type) if geom_type else None,
                fiona_schema=schema,
            ))

    return DatasetInfo(name=dataset_name, layers=layers)


def generate_sources_yml(
    dataset: DatasetInfo,
    staging_bucket: str,
    staging_prefix: str,
) -> str:
    """Generate dbt _sources.yml content for a dataset."""
    prefix = staging_prefix.strip("/")
    tables = []

    for layer in dataset.layers:
        ext = layer.file_extension
        base_path = f"gs://{staging_bucket}/{prefix}/{dataset.name}/{layer.name}"

        external_config = {
            "location": f"{base_path}/data_drop=*/data.{ext}",
            "options": {
                "format": "JSON",
                "hive_partition_uri_prefix": f"{base_path}/",
            },
        }

        if ext == "geojsonl":
            external_config["options"]["json_extension"] = "GEOJSON"

        columns = []
        for col in layer.bq_columns:
            col_def = {
                "name": col["name"],
                "data_type": col["data_type"],
            }
            if col["source_type"] is not None:
                col_def["description"] = f"Source: {col['name']} ({col['source_type']})"
                col_def["meta"] = {"rename": None}
            columns.append(col_def)

        table_def = {
            "name": layer.name,
            "description": f"Layer: {layer.name} ({layer.geometry_type or 'no geometry'})",
            "external": external_config,
            "columns": columns,
        }
        tables.append(table_def)

    sources_doc = {
        "version": 2,
        "sources": [
            {
                "name": dataset.name,
                "description": f"Raw geodata from {dataset.name}.gdb",
                "tables": tables,
            }
        ],
    }

    return yaml.dump(sources_doc, default_flow_style=False, sort_keys=False)


def generate_staging_sql(dataset_name: str, layer: LayerInfo) -> str:
    """Generate a dbt staging SQL model for a layer."""
    model_lines = [
        f"-- stg_{dataset_name}_{layer.name}.sql",
        "-- Auto-generated by rextag scan. Edit column renames in _sources.yml meta.rename.",
        "",
        "{{",
        "    config(",
        "        materialized='view',",
        "    )",
        "}}",
        "",
        f"select * from {{{{ source('{dataset_name}', '{layer.name}') }}}}",
        "",
    ]
    return "\n".join(model_lines)


def generate_dbt_files(
    dataset: DatasetInfo,
    output_dir: Path,
    staging_bucket: str,
    staging_prefix: str,
) -> Path:
    """Write dbt source YAML and staging SQL files for a dataset."""
    dataset_dir = output_dir / dataset.name
    dataset_dir.mkdir(parents=True, exist_ok=True)

    sources_content = generate_sources_yml(dataset, staging_bucket, staging_prefix)
    (dataset_dir / "_sources.yml").write_text(sources_content)

    for layer in dataset.layers:
        sql_content = generate_staging_sql(dataset.name, layer)
        filename = f"stg_{dataset.name}_{layer.name}.sql"
        (dataset_dir / filename).write_text(sql_content)

    return dataset_dir
